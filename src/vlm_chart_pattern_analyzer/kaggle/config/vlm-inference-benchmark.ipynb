{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "# Read configuration from injected cells or environment\n",
        "MODEL_ID = globals().get('CONFIG_MODEL_ID') or os.environ.get('MODEL_ID', 'SmolVLM2-2.2B')\n",
        "BENCHMARK_PRECISION = globals().get('CONFIG_PRECISION') or os.environ.get('PRECISION', 'fp32')\n",
        "TENSORRT = globals().get('CONFIG_TENSORRT', False) or os.environ.get('TENSORRT', 'False').lower() == 'true'\n",
        "RESULT_DIR = globals().get('CONFIG_OUTPUT_SUBDIR') or os.environ.get('OUTPUT_SUBDIR', 'results')\n",
        "IMAGE_LIMIT = globals().get('CONFIG_IMAGE_LIMIT', 0) or int(os.environ.get('IMAGE_LIMIT', '0'))\n",
        "\n",
        "# Debug printout\n",
        "print('=== INJECTED CONFIGURATION ===')\n",
        "print(f'MODEL_ID: {MODEL_ID}')\n",
        "print(f'BENCHMARK_PRECISION: {BENCHMARK_PRECISION}')\n",
        "print(f'TENSORRT: {TENSORRT}')\n",
        "print(f'RESULT_DIR: {RESULT_DIR}')\n",
        "print(f'IMAGE_LIMIT: {IMAGE_LIMIT}')\n",
        "print(f'TORCH_VERSION: {torch.__version__}')\n",
        "print(f'CUDA_AVAILABLE: {torch.cuda.is_available()}')\n",
        "print('===========================\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "    \n",
        "    with open(os.path.join(RESULT_DIR, 'notebook_boot.txt'), 'w', encoding='utf-8') as f:\n",
        "        f.write('Notebook boot test: kernel started successfully.')\n",
        "    \n",
        "    with open(os.path.join(RESULT_DIR, 'notebook_test.txt'), 'w', encoding='utf-8') as f:\n",
        "        f.write('Notebook file write test successful.')\n",
        "    \n",
        "    print(f'NOTEBOOK BOOT: Successfully created test files in {RESULT_DIR}')\n",
        "except Exception as e:\n",
        "    print(f'NOTEBOOK BOOT ERROR: {e}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from huggingface_hub import login, get_token\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL LOADING CELL STARTED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "hf_token = os.environ.get('HUGGINGFACE_HUB_TOKEN', '')\n",
        "print(f\"HF_TOKEN in environment: {bool(hf_token)}\")\n",
        "print(f\"MODEL_ID: {MODEL_ID}\")\n",
        "print(f\"BENCHMARK_PRECISION: {BENCHMARK_PRECISION}\")\n",
        "\n",
        "processor = None\n",
        "model = None\n",
        "model_load_error = None\n",
        "\n",
        "# Check if MODEL_ID is a local path (starts with / or has : for Windows)\n",
        "is_local_path = (\n",
        "    MODEL_ID.startswith('/') or \n",
        "    (len(MODEL_ID) > 2 and MODEL_ID[1] == ':')  # Windows absolute path like C:\\\n",
        ")\n",
        "\n",
        "if is_local_path:\n",
        "    # Normalize Windows backslashes to forward slashes for from_pretrained()\n",
        "    model_path = MODEL_ID.replace('\\\\', '/')\n",
        "    print(f\"Detected local model path: {model_path}\")\n",
        "else:\n",
        "    model_path = MODEL_ID\n",
        "\n",
        "try:\n",
        "    if hf_token and not is_local_path:\n",
        "        print(f'\\nAuthenticating with HuggingFace token...')\n",
        "        login(token=hf_token, add_to_git_credential=False)\n",
        "        print(f'✓ Authentication successful')\n",
        "    elif is_local_path:\n",
        "        print('Loading from local path - no HF authentication needed')\n",
        "    else:\n",
        "        print('⚠ No HuggingFace token - loading public models only')\n",
        "    \n",
        "    from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "    \n",
        "    print(f'\\n1. Loading processor for {model_path}...')\n",
        "    try:\n",
        "        processor = AutoProcessor.from_pretrained(\n",
        "            model_path, \n",
        "            trust_remote_code=True,\n",
        "            token=hf_token if (hf_token and not is_local_path) else None\n",
        "        )\n",
        "        print('✓ Processor loaded successfully')\n",
        "    except Exception as proc_err:\n",
        "        error_msg = f'{type(proc_err).__name__}: {str(proc_err)[:300]}'\n",
        "        print(f'✗ ERROR loading processor: {error_msg}')\n",
        "        model_load_error = error_msg\n",
        "        processor = None\n",
        "    \n",
        "    print(f'\\n2. Loading model in {BENCHMARK_PRECISION}...')\n",
        "    try:\n",
        "        load_in_fp16 = (BENCHMARK_PRECISION == 'fp16')\n",
        "        model = AutoModelForImageTextToText.from_pretrained(\n",
        "            model_path,\n",
        "            trust_remote_code=True,\n",
        "            token=hf_token if (hf_token and not is_local_path) else None,\n",
        "            torch_dtype='auto' if load_in_fp16 else None\n",
        "        )\n",
        "        print('✓ Model loaded successfully')\n",
        "    except Exception as model_err:\n",
        "        error_msg = f'{type(model_err).__name__}: {str(model_err)[:300]}'\n",
        "        print(f'✗ ERROR loading model: {error_msg}')\n",
        "        model_load_error = error_msg\n",
        "        model = None\n",
        "    \n",
        "    if processor is not None and model is not None:\n",
        "        print(f'\\n✓✓ SUCCESS: Both processor and model loaded!')\n",
        "    else:\n",
        "        print(f'\\n✗✗ FAILURE: processor={processor is not None}, model={model is not None}')\n",
        "        if model_load_error:\n",
        "            print(f'Latest error: {model_load_error}')\n",
        "        \n",
        "except Exception as e:\n",
        "    error_msg = f'{type(e).__name__}: {str(e)[:300]}'\n",
        "    print(f'\\n✗ CRITICAL ERROR: {error_msg}')\n",
        "    model_load_error = error_msg\n",
        "    processor = None\n",
        "    model = None\n",
        "\n",
        "print(\"=\"*60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!{sys.executable} -m pip install transformers==4.57.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!{sys.executable} -m pip show torch\n",
        "!{sys.executable} -m pip show transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from PIL import Image\n",
        "    import time\n",
        "    \n",
        "    # Create a dummy image for testing\n",
        "    dummy_img = Image.new('RGB', (224, 224), color='red')\n",
        "    \n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    messages = [{'role': 'user', 'content': [{'type': 'image', 'path': '<PIL Image>'}, {'type': 'text', 'text': 'Describe this image'}]}]\n",
        "    \n",
        "    # This will likely fail due to PIL image in path, but we'll handle it\n",
        "    print(f'Testing inference on {device}...')\n",
        "except Exception as e:\n",
        "    print(f'Warning during test: {e}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "csv_path = os.path.join(RESULT_DIR, 'benchmark_results.csv')\n",
        "os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "\n",
        "# Create results header\n",
        "fieldnames = [\n",
        "    'image_filename', 'model_id', 'precision', 'tensorrt', 'device',\n",
        "    'latency_ms', 'memory_used_mb', 'tokens_generated', 'throughput_tokens_per_sec',\n",
        "    'timestamp', 'response', 'error'\n",
        "]\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Determine images to process\n",
        "images_dir = os.environ.get('CHART_IMAGES_DIR', '/kaggle/input/chart-patterns')\n",
        "image_paths = []\n",
        "if os.path.isdir(images_dir):\n",
        "    for root, _, files in os.walk(images_dir):\n",
        "        for fn in files:\n",
        "            if fn.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "                image_paths.append(os.path.join(root, fn))\n",
        "else:\n",
        "    # fallback: if a single file path provided\n",
        "    if os.path.isfile(images_dir):\n",
        "        image_paths = [images_dir]\n",
        "\n",
        "# Respect image limit (0 means no limit)\n",
        "limit = int(globals().get('CONFIG_IMAGE_LIMIT') or os.environ.get('IMAGE_LIMIT', '0'))\n",
        "if limit > 0:\n",
        "    image_paths = image_paths[:limit]\n",
        "\n",
        "results = []\n",
        "\n",
        "# Try to ensure processor/model are available (some earlier cell may have loaded them)\n",
        "try:\n",
        "    processor  # noqa: F821\n",
        "except Exception:\n",
        "    processor = None\n",
        "try:\n",
        "    model  # noqa: F821\n",
        "except Exception:\n",
        "    model = None\n",
        "\n",
        "# Attempt to load processor/model if not present\n",
        "if processor is None or model is None:\n",
        "    try:\n",
        "        from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "        processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "        model = AutoModelForImageTextToText.from_pretrained(MODEL_ID)\n",
        "    except Exception as e:\n",
        "        print(f'Warning: Could not load model/processor: {e}')\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Process each image and run inference if model loaded\n",
        "for img_path in image_paths:\n",
        "    row = {\n",
        "        'image_filename': os.path.basename(img_path),\n",
        "        'model_id': MODEL_ID,\n",
        "        'precision': BENCHMARK_PRECISION,\n",
        "        'tensorrt': str(TENSORRT),\n",
        "        'device': device,\n",
        "        'latency_ms': None,\n",
        "        'memory_used_mb': None,\n",
        "        'tokens_generated': None,\n",
        "        'throughput_tokens_per_sec': None,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'response': None,\n",
        "        'error': None\n",
        "    }\n",
        "    try:\n",
        "        from PIL import Image\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        # Prepare input\n",
        "        if processor is None or model is None:\n",
        "            error_detail = f'processor={processor is not None}, model={model is not None}'\n",
        "            if model_load_error:\n",
        "                error_detail += f', load_error={model_load_error}'\n",
        "            raise RuntimeError(f'Model or processor not available ({error_detail})')\n",
        "        inputs = processor(images=img, return_tensors='pt')\n",
        "        device_t = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        model.to(device_t)\n",
        "        # Move tensors to device\n",
        "        inputs = {k: v.to(device_t) for k, v in inputs.items()}\n",
        "        import time\n",
        "        torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None\n",
        "        start = time.perf_counter()\n",
        "        if hasattr(model, 'generate'):\n",
        "            gen = model.generate(**inputs, max_new_tokens=128)\n",
        "            # Attempt to decode using processor if possible\n",
        "            try:\n",
        "                response = processor.batch_decode(gen, skip_special_tokens=True)[0]\n",
        "            except Exception:\n",
        "                response = str(gen)\n",
        "            tokens = int(gen.shape[-1]) if hasattr(gen, 'shape') else None\n",
        "        else:\n",
        "            out = model(**inputs)\n",
        "            response = str(out)\n",
        "            tokens = None\n",
        "        end = time.perf_counter()\n",
        "        latency_ms = (end - start) * 1000.0\n",
        "        memory_mb = None\n",
        "        if torch.cuda.is_available():\n",
        "            memory_mb = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n",
        "        throughput = (tokens / (end - start)) if tokens and (end - start) > 0 else None\n",
        "        row['latency_ms'] = round(latency_ms, 3)\n",
        "        row['memory_used_mb'] = round(memory_mb, 3) if memory_mb is not None else None\n",
        "        row['tokens_generated'] = int(tokens) if tokens is not None else None\n",
        "        row['throughput_tokens_per_sec'] = round(throughput, 3) if throughput is not None else None\n",
        "        row['response'] = response\n",
        "    except Exception as e:\n",
        "        row['error'] = str(e)\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    results.append(row)\n",
        "\n",
        "# Write CSV\n",
        "try:\n",
        "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "    print(f'Successfully wrote {len(results)} results to {csv_path}')\n",
        "except Exception as e:\n",
        "    print(f'ERROR writing CSV: {e}')\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'\\nBenchmark complete!')\n",
        "print(f'Results saved to: {csv_path}')\n",
        "print(f'Device: {device}')\n",
        "print(f'Model: {MODEL_ID}')\n",
        "print(f'Precision: {BENCHMARK_PRECISION}')\n",
        "print(f'TensorRT: {TENSORRT}')\n",
        "print(f'Output dir: {RESULT_DIR}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
