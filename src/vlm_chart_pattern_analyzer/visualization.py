"""Plotly visualization utilities for benchmark results."""
from typing import Dict, List, Any
from pathlib import Path
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px


def load_benchmark_results(csv_path: str) -> pd.DataFrame:
    """Load benchmark results from CSV."""
    df = pd.read_csv(csv_path)
    df['latency_ms'] = pd.to_numeric(df['latency_ms'])
    df['memory_mb'] = pd.to_numeric(df['memory_mb'])
    return df


def plot_latency_by_model_precision(df: pd.DataFrame, output_path: str = None) -> go.Figure:
    """Create box plot of latency by model and precision."""
    fig = px.box(
        df,
        x='model',
        y='latency_ms',
        color='precision',
        title='Inference Latency by Model and Precision',
        labels={'latency_ms': 'Latency (ms)', 'model': 'Model', 'precision': 'Precision'},
        height=600,
        template='plotly_white'
    )
    
    fig.update_layout(
        font=dict(size=12),
        hovermode='x unified',
        showlegend=True
    )
    
    if output_path:
        fig.write_html(output_path)
        print(f"‚úì Saved: {output_path}")
    
    return fig


def plot_memory_by_model_precision(df: pd.DataFrame, output_path: str = None) -> go.Figure:
    """Create box plot of memory usage by model and precision."""
    fig = px.box(
        df,
        x='model',
        y='memory_mb',
        color='precision',
        title='Memory Usage by Model and Precision',
        labels={'memory_mb': 'Memory (MB)', 'model': 'Model', 'precision': 'Precision'},
        height=600,
        template='plotly_white'
    )
    
    fig.update_layout(
        font=dict(size=12),
        hovermode='x unified',
        showlegend=True
    )
    
    if output_path:
        fig.write_html(output_path)
        print(f"‚úì Saved: {output_path}")
    
    return fig


def plot_latency_vs_memory(df: pd.DataFrame, output_path: str = None) -> go.Figure:
    """Create scatter plot of latency vs memory usage."""
    fig = px.scatter(
        df,
        x='memory_mb',
        y='latency_ms',
        color='model',
        symbol='precision',
        title='Latency vs Memory Usage',
        labels={'memory_mb': 'Memory (MB)', 'latency_ms': 'Latency (ms)', 'model': 'Model'},
        height=600,
        template='plotly_white',
        hover_data=['image', 'timestamp']
    )
    
    fig.update_traces(marker=dict(size=10))
    fig.update_layout(
        font=dict(size=12),
        hovermode='closest',
        showlegend=True
    )
    
    if output_path:
        fig.write_html(output_path)
        print(f"‚úì Saved: {output_path}")
    
    return fig


def plot_tokens_generated(df: pd.DataFrame, output_path: str = None) -> go.Figure:
    """Create bar plot of tokens generated by model."""
    tokens_by_model = df.groupby(['model', 'precision'])['tokens'].mean().reset_index()
    
    fig = px.bar(
        tokens_by_model,
        x='model',
        y='tokens',
        color='precision',
        title='Average Tokens Generated by Model and Precision',
        labels={'tokens': 'Tokens', 'model': 'Model', 'precision': 'Precision'},
        height=600,
        template='plotly_white',
        barmode='group'
    )
    
    fig.update_layout(
        font=dict(size=12),
        showlegend=True
    )
    
    if output_path:
        fig.write_html(output_path)
        print(f"‚úì Saved: {output_path}")
    
    return fig


def plot_model_comparison_heatmap(df: pd.DataFrame, metric: str = 'latency_ms', output_path: str = None) -> go.Figure:
    """Create heatmap comparing models and precisions on a metric."""
    metric_col = metric if metric in df.columns else 'latency_ms'
    
    pivot_data = df.pivot_table(
        values=metric_col,
        index='model',
        columns='precision',
        aggfunc='mean'
    )
    
    fig = go.Figure(
        data=go.Heatmap(
            z=pivot_data.values,
            x=pivot_data.columns,
            y=pivot_data.index,
            colorscale='Viridis',
            hovertemplate='Model: %{y}<br>Precision: %{x}<br>' + metric_col + ': %{z:.2f}<extra></extra>'
        )
    )
    
    fig.update_layout(
        title=f'{metric_col.replace("_", " ").title()} Heatmap: Models vs Precisions',
        xaxis_title='Precision',
        yaxis_title='Model',
        height=500,
        template='plotly_white'
    )
    
    if output_path:
        fig.write_html(output_path)
        print(f"‚úì Saved: {output_path}")
    
    return fig


def plot_comprehensive_dashboard(df: pd.DataFrame, output_dir: str = 'data/results/visualizations') -> None:
    """Create comprehensive dashboard with all visualizations."""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\nüìä Generating Plotly visualizations in {output_dir}/...")
    
    # Individual plots
    plot_latency_by_model_precision(df, str(output_path / 'latency_by_model.html'))
    plot_memory_by_model_precision(df, str(output_path / 'memory_by_model.html'))
    plot_latency_vs_memory(df, str(output_path / 'latency_vs_memory.html'))
    plot_tokens_generated(df, str(output_path / 'tokens_generated.html'))
    plot_model_comparison_heatmap(df, 'latency_ms', str(output_path / 'heatmap_latency.html'))
    plot_model_comparison_heatmap(df, 'memory_mb', str(output_path / 'heatmap_memory.html'))
    
    print(f"‚úì All visualizations created in {output_dir}/")


def create_summary_statistics(df: pd.DataFrame, output_path: str = 'data/results/summary.txt') -> None:
    """Generate summary statistics from benchmark results."""
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    summary = []
    summary.append("=" * 80)
    summary.append("VLM BENCHMARK SUMMARY")
    summary.append("=" * 80)
    summary.append("")
    
    # Overall statistics
    summary.append("OVERALL STATISTICS:")
    summary.append(f"  Total benchmarks: {len(df)}")
    summary.append(f"  Models tested: {', '.join(df['model'].unique())}")
    summary.append(f"  Precisions tested: {', '.join(df['precision'].unique())}")
    summary.append("")
    
    # Per-model statistics
    summary.append("PER-MODEL STATISTICS:")
    summary.append("")
    
    for model in sorted(df['model'].unique()):
        model_df = df[df['model'] == model]
        summary.append(f"  {model.upper()}:")
        summary.append(f"    Avg Latency: {model_df['latency_ms'].mean():.2f} ms")
        summary.append(f"    Min Latency: {model_df['latency_ms'].min():.2f} ms")
        summary.append(f"    Max Latency: {model_df['latency_ms'].max():.2f} ms")
        summary.append(f"    Avg Memory:  {model_df['memory_mb'].mean():.2f} MB")
        summary.append(f"    Avg Tokens:  {model_df['tokens'].mean():.0f}")
        
        # Per-precision stats
        for precision in sorted(model_df['precision'].unique()):
            prec_df = model_df[model_df['precision'] == precision]
            summary.append(f"      {precision}: latency={prec_df['latency_ms'].mean():.2f}ms, memory={prec_df['memory_mb'].mean():.2f}MB")
        summary.append("")
    
    # Best performers
    summary.append("BEST PERFORMERS:")
    summary.append(f"  Fastest: {df.loc[df['latency_ms'].idxmin(), 'model']} ({df['latency_ms'].min():.2f}ms)")
    summary.append(f"  Most Memory Efficient: {df.loc[df['memory_mb'].idxmin(), 'model']} ({df['memory_mb'].min():.2f}MB)")
    summary.append(f"  Most Tokens: {df.loc[df['tokens'].idxmax(), 'model']} ({df['tokens'].max():.0f} tokens)")
    summary.append("")
    summary.append("=" * 80)
    
    # Write to file
    with open(output_file, 'w') as f:
        f.write('\n'.join(summary))
    
    print(f"‚úì Summary saved to {output_path}")
    
    # Also print to console
    print('\n' + '\n'.join(summary))


def plot_similarity_heatmap(similarity_dict: Dict, output_path: str = None) -> go.Figure:
    """Create heatmap of model-to-model similarity."""
    import numpy as np
    
    if not similarity_dict:
        print("‚ö†Ô∏è No similarity data to visualize")
        return None
    
    # Extract data from first entry to get models
    first_entry = list(similarity_dict.values())[0]
    models = first_entry['models']
    
    # Compute average similarity matrix
    all_matrices = []
    for entry in similarity_dict.values():
        all_matrices.append(np.array(entry['similarity_matrix']))
    
    avg_matrix = np.mean(all_matrices, axis=0)
    
    fig = go.Figure(
        data=go.Heatmap(
            z=avg_matrix,
            x=models,
            y=models,
            colorscale='RdBu',
            zmid=0.0,
            colorbar=dict(title='Similarity'),
            hovertemplate='%{y} vs %{x}<br>Similarity: %{z:.3f}<extra></extra>'
        )
    )
    
    fig.update_layout(
        title='Average Model Similarity Across All Images',
        xaxis_title='Model',
        yaxis_title='Model',
        height=600,
        template='plotly_white',
        font=dict(size=12)
    )
    
    if output_path:
        fig.write_html(output_path)
        print(f"‚úì Saved: {output_path}")
    
    return fig


def plot_model_agreement_bars(agreement_dict: Dict, output_path: str = None) -> go.Figure:
    """Create bar chart of model pair agreement."""
    if not agreement_dict:
        print("‚ö†Ô∏è No agreement data to visualize")
        return None
    
    pairs = list(agreement_dict.keys())
    means = [agreement_dict[pair]['mean_similarity'] for pair in pairs]
    stds = [agreement_dict[pair]['std_similarity'] for pair in pairs]
    
    fig = go.Figure(
        data=[
            go.Bar(
                x=pairs,
                y=means,
                error_y=dict(type='data', array=stds),
                marker_color='steelblue',
                hovertemplate='%{x}<br>Avg Similarity: %{y:.3f}<br>Std: %{error_y.array:.3f}<extra></extra>'
            )
        ]
    )
    
    fig.update_layout(
        title='Model Pair Agreement (Mean Similarity)',
        xaxis_title='Model Pair',
        yaxis_title='Similarity',
        height=600,
        template='plotly_white',
        font=dict(size=11),
        xaxis_tickangle=-45,
        showlegend=False
    )
    
    if output_path:
        fig.write_html(output_path)
        print(f"‚úì Saved: {output_path}")
    
    return fig


def plot_similarity_distribution(similarity_dict: Dict, output_path: str = None) -> go.Figure:
    """Create distribution plot of similarities across images."""
    import numpy as np
    
    if not similarity_dict:
        print("‚ö†Ô∏è No similarity data to visualize")
        return None
    
    all_similarities = []
    for entry in similarity_dict.values():
        # Extract upper triangle (avoid duplicates)
        matrix = np.array(entry['similarity_matrix'])
        upper_triangle = matrix[np.triu_indices_from(matrix, k=1)]
        all_similarities.extend(upper_triangle)
    
    fig = px.histogram(
        x=all_similarities,
        nbins=30,
        title='Distribution of Model Similarities Across All Images',
        labels={'x': 'Similarity Score', 'y': 'Frequency'},
        template='plotly_white'
    )
    
    fig.update_layout(
        height=600,
        font=dict(size=12),
        showlegend=False
    )
    
    if output_path:
        fig.write_html(output_path)
        print(f"‚úì Saved: {output_path}")
    
    return fig

