# Step 8: Plotly Visualizations

## Overview

Added interactive Plotly-based visualizations to analyze VLM benchmark performance across different models and precision levels. The visualization suite provides multiple interactive charts for comprehensive performance analysis.

## Features

### Visualization Module (`src/vlm_chart_pattern_analyzer/visualization.py`)

Core functions for generating interactive Plotly charts:

1. **plot_latency_by_model_precision()** - Box plot comparing latency across models and precisions
2. **plot_memory_by_model_precision()** - Box plot comparing memory usage across models and precisions
3. **plot_latency_vs_memory()** - Scatter plot showing latency vs memory tradeoffs
4. **plot_tokens_generated()** - Bar chart of average tokens generated by model and precision
5. **plot_model_comparison_heatmap()** - Heatmap comparing models and precisions on any metric
6. **plot_comprehensive_dashboard()** - Generates all visualizations at once
7. **create_summary_statistics()** - Text summary with best performers and key metrics
8. **load_benchmark_results()** - Load CSV benchmark data

### CLI Interface (`scripts/visualize.py`)

Command-line tool to generate visualizations from benchmark CSV results:

```bash
poetry run python scripts/visualize.py [OPTIONS]
```

#### Options

- `--input INPUT` - Path to benchmark CSV file (default: `data/results/benchmark.csv`)
- `--output OUTPUT` - Output directory for visualizations (default: `data/results/visualizations`)
- `--summary SUMMARY` - Output path for summary statistics (default: `data/results/summary.txt`)
- `--no-dashboard` - Skip comprehensive dashboard generation
- `--no-summary` - Skip summary statistics generation

#### Examples

```bash
# Generate visualizations from default benchmark.csv
poetry run python scripts/visualize.py

# Use custom input and output paths
poetry run python scripts/visualize.py --input results.csv --output ./charts

# Generate only summary statistics
poetry run python scripts/visualize.py --no-dashboard

# Generate only dashboard (skip summary)
poetry run python scripts/visualize.py --no-summary
```

## Usage Workflow

### 1. Generate Charts

```bash
poetry run python scripts/generate_charts.py --num-charts 5
```

### 2. Run Benchmarks

```bash
# Benchmark single model/precision
poetry run python scripts/benchmark.py --model qwen2-vl-2b --precision fp32 --limit 5

# Or benchmark all models with all precisions (see benchmark.py for options)
poetry run python scripts/benchmark.py
```

### 3. Generate Visualizations

```bash
poetry run python scripts/visualize.py
```

### 4. View Results

Open the generated HTML files in your browser:
- `data/results/visualizations/latency_by_model.html` - Latency comparison
- `data/results/visualizations/memory_by_model.html` - Memory comparison
- `data/results/visualizations/latency_vs_memory.html` - Tradeoff analysis
- `data/results/visualizations/tokens_generated.html` - Token output comparison
- `data/results/visualizations/heatmap_latency.html` - Performance heatmap
- `data/results/visualizations/heatmap_memory.html` - Memory heatmap
- `data/results/summary.txt` - Summary statistics

## Generated Visualizations

### Latency by Model
- Box plot showing latency distribution for each model-precision combination
- Interactive hover to see exact values
- Useful for identifying consistent and variable performance

### Memory by Model
- Box plot showing memory usage distribution
- Helps identify memory-efficient configurations
- Interactive filtering and selection

### Latency vs Memory
- Scatter plot showing the latency-memory tradeoff
- Color-coded by model, symbol-coded by precision
- Identifies optimal configurations for specific use cases

### Tokens Generated
- Bar chart comparing average token output per model-precision combination
- Grouped by precision level
- Shows model capability differences

### Heatmap Comparisons
- Two heatmaps: one for latency, one for memory
- Model rows Ã— Precision columns
- Color intensity shows performance levels
- Quick visual comparison across all combinations

### Summary Statistics
- Text report with overall statistics
- Per-model performance breakdown
- Per-precision analysis for each model
- Best performers highlighting

## Technical Details

### Dependencies

- **plotly** (5.18.0+) - Interactive visualization library
- **pandas** - Data manipulation and pivot operations

### Data Format

Benchmark CSV files must contain columns:
- `timestamp` - When benchmark was run
- `image` - Image filename
- `model` - Model name (qwen2-vl-2b, llava-1.6-8b, phi-3-vision)
- `precision` - Precision level (fp32, fp16, int8)
- `latency_ms` - Inference latency in milliseconds
- `memory_mb` - Peak memory usage in MB
- `tokens` - Number of tokens generated

### Output Format

- **HTML Files** - Interactive Plotly charts (4.5-5MB each)
  - Fully self-contained
  - Interactive zoom, pan, hover
  - Download as PNG via Plotly toolbar
  - No external dependencies needed to view

- **Summary Text** - Human-readable statistics report
  - ASCII-formatted for terminal display
  - Comprehensive model/precision breakdown
  - Best performers highlighted

## Performance Considerations

- Visualizations render client-side in browser
- HTML files can be large (4-5MB) due to Plotly library inclusion
- Interactive features work in all modern browsers
- No server required for viewing HTML files

## Future Enhancements

Potential improvements for visualization suite:
1. Static image export (PNG/SVG) via optional Kaleido integration
2. Web dashboard with live updating from running benchmarks
3. Time-series tracking of performance across benchmark runs
4. Comparison mode between different benchmark runs
5. Custom metric combinations in heatmaps
6. Export to PDF reports

## Testing

Test visualizations with sample data:

```bash
poetry run python scripts/test_visualizations.py
```

This generates test visualizations in `data/results/test_visualizations/` for validation.

## Integration with CI/CD

The visualization workflow can be integrated into Docker deployments:

```dockerfile
# Generate benchmark
RUN poetry run python scripts/benchmark.py --limit 5

# Generate visualizations
RUN poetry run python scripts/visualize.py

# Store results in volume
VOLUME ["/app/data/results"]
```

Results can then be accessed from the container's `/app/data/results/` directory.
